{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvE3H0wkj23S"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgpdUm9rlvRU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX9-DPRambsP"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/underwater/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZEQXl9ejvoy"
      },
      "outputs": [],
      "source": [
        "def visualize_results(targets_denorm, predictions_denorm):\n",
        "    # Create subplots for scatter plots\n",
        "    plt.figure(figsize=(18, 20))\n",
        "\n",
        "    # Scatter plots of True vs Predicted values\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        true = targets_denorm[:, i]\n",
        "        pred = predictions_denorm[:, i]\n",
        "        plt.scatter(true, pred, alpha=0.3, label='Samples')\n",
        "        plt.plot([min(true), max(true)], [min(true), max(true)], 'r--', label='Perfect Prediction')\n",
        "        plt.xlabel(f'True Value (Thruster {i+1})')\n",
        "        plt.ylabel(f'Predicted Value (Thruster {i+1})')\n",
        "        plt.title(f'Thruster {i+1} - True vs Predicted')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add R² and MAE to plot\n",
        "        r2 = r2_score(true, pred)\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "        plt.text(0.05, 0.9, f'R²: {r2:.2f}\\nMAE: {mae:.2f}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('true_vs_predicted_scatter.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Create sample comparison plot (first 100 samples)\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    sample_indices = np.arange(100)\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        plt.plot(sample_indices, targets_denorm[:100, i], 'b-', label='True')\n",
        "        plt.plot(sample_indices, predictions_denorm[:100, i], 'r--', label='Predicted')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'Thruster {i+1} - First 100 Samples')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Residuals distribution\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    residuals = targets_denorm - predictions_denorm\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        plt.hist(residuals[:, i], bins=50, alpha=0.7)\n",
        "        plt.xlabel('Residual (True - Predicted)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'Thruster {i+1} - Residual Distribution')\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('residual_distribution.png')\n",
        "    plt.show()\n",
        "\n",
        "# Configuration for feature normalization\n",
        "class FeatureNormalizer:\n",
        "    def __init__(self, x_scales, y_scales):\n",
        "        self.x_scales = np.array(x_scales, dtype=np.float32)\n",
        "        self.y_scales = np.array(y_scales, dtype=np.float32)\n",
        "\n",
        "    def normalize_x(self, x):\n",
        "        return x / self.x_scales\n",
        "\n",
        "    def denormalize_x(self, x_norm):\n",
        "        return x_norm * self.x_scales\n",
        "\n",
        "    def normalize_y(self, y):\n",
        "        return y / self.y_scales\n",
        "\n",
        "    def denormalize_y(self, y_norm):\n",
        "        return y_norm * self.y_scales\n",
        "\n",
        "NU_MIN: float  = 0.0\n",
        "NU_MAX: float  = 5.0\n",
        "D_LOC_MAX: float  = 5.0\n",
        "DEPTH_MIN: float  = 2.0\n",
        "DEPTH_MAX: float  = 25.0\n",
        "ZERO: float = 1.0\n",
        "\n",
        "# Example scaling configuration\n",
        "X_SCALE_FACTORS = [\n",
        "    # Current state features\n",
        "    ZERO, ZERO, DEPTH_MAX,\n",
        "    np.pi/4, np.pi/4, np.pi,\n",
        "    NU_MAX, NU_MAX, NU_MAX,\n",
        "    0.05, 0.05, 0.1,\n",
        "    # Desired state features\n",
        "    D_LOC_MAX, D_LOC_MAX, DEPTH_MAX,\n",
        "    ZERO, ZERO, np.pi,\n",
        "    NU_MAX, NU_MAX, ZERO,\n",
        "    ZERO, ZERO, ZERO\n",
        "]\n",
        "\n",
        "Y_SCALE_FACTORS = [80.0, 80.0, 80.0, 80.0,  # Main thrusters\n",
        "                   50.0, 50.0, 50.0, 50.0]   # Tunnel thrusters\n",
        "normalizer = FeatureNormalizer(X_SCALE_FACTORS, Y_SCALE_FACTORS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8IKP7W-mFcq"
      },
      "outputs": [],
      "source": [
        "# Load and process data\n",
        "with h5py.File('mpc_data.h5', 'r') as hf:\n",
        "    X = np.hstack((hf['x_current'][:], hf['x_desired'][:])).astype(np.float32)\n",
        "    y = hf['u_opt'][:].astype(np.float32)\n",
        "\n",
        "# Split dataset into train (60%), val (20%), test (20%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, shuffle=True, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Apply normalization\n",
        "X_train_norm = normalizer.normalize_x(X_train)\n",
        "X_val_norm = normalizer.normalize_x(X_val)\n",
        "X_test_norm = normalizer.normalize_x(X_test)\n",
        "y_train_norm = normalizer.normalize_y(y_train)\n",
        "y_val_norm = normalizer.normalize_y(y_val)\n",
        "y_test_norm = normalizer.normalize_y(y_test)\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train_norm), torch.FloatTensor(y_train_norm))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val_norm), torch.FloatTensor(y_val_norm))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test_norm), torch.FloatTensor(y_test_norm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV31Mvs2mPjz"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayDmuZllmU5M"
      },
      "outputs": [],
      "source": [
        "# Network Architecture (unchanged)\n",
        "class FossenNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FossenNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(24, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Linear(64, 8)\n",
        "        )\n",
        "\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
        "                nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    'batch_size': 256,\n",
        "    'lr': 3e-4,\n",
        "    'epochs': 5,\n",
        "    'weight_decay': 1e-6,\n",
        "    'patience': 15\n",
        "}\n",
        "\n",
        "def train():\n",
        "    model = FossenNet()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.5)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                pred = model(x_val)\n",
        "                val_loss += criterion(pred, y_val).item()\n",
        "\n",
        "        avg_train = train_loss/len(train_loader)\n",
        "        avg_val = val_loss/len(val_loader)\n",
        "        scheduler.step(avg_val)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val < best_loss:\n",
        "            best_loss = avg_val\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= config['patience']:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['epochs']} | \"\n",
        "              f\"Train: {avg_train:.4f} | Val: {avg_val:.4f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "def evaluate_test_set():\n",
        "    model = FossenNet()\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            pred = model(x_batch)\n",
        "            predictions.append(pred.numpy())\n",
        "            targets.append(y_batch.numpy())\n",
        "\n",
        "    predictions = np.vstack(predictions)\n",
        "    targets = np.vstack(targets)\n",
        "\n",
        "    # Denormalize\n",
        "    predictions_denorm = normalizer.denormalize_y(predictions)\n",
        "    targets_denorm = normalizer.denormalize_y(targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'MAE': mean_absolute_error(targets_denorm, predictions_denorm),\n",
        "        'MSE': mean_squared_error(targets_denorm, predictions_denorm),\n",
        "        'RMSE': np.sqrt(mean_squared_error(targets_denorm, predictions_denorm)),\n",
        "        'R2': r2_score(targets_denorm, predictions_denorm)\n",
        "    }\n",
        "\n",
        "    # Per-thruster metrics\n",
        "    thruster_metrics = []\n",
        "    for i in range(8):\n",
        "        thruster_metrics.append({\n",
        "            'Thruster': i+1,\n",
        "            'MAE': mean_absolute_error(targets_denorm[:, i], predictions_denorm[:, i]),\n",
        "            'MSE': mean_squared_error(targets_denorm[:, i], predictions_denorm[:, i]),\n",
        "            'RMSE': np.sqrt(mean_squared_error(targets_denorm[:, i], predictions_denorm[:, i])),\n",
        "            'R2': r2_score(targets_denorm[:, i], predictions_denorm[:, i])\n",
        "        })\n",
        "\n",
        "    return metrics, thruster_metrics, predictions_denorm, targets_denorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEYj5o5FxURO"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgs7lnmemWtu"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    train()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    metrics, thruster_metrics, preds, targets = evaluate_test_set()\n",
        "\n",
        "    print(\"\\nFinal Test Set Metrics:\")\n",
        "    print(f\"MAE: {metrics['MAE']:.4f}\")\n",
        "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "    print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"R²: {metrics['R2']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-Thruster Metrics:\")\n",
        "    for tm in thruster_metrics:\n",
        "        print(f\"\\nThruster {tm['Thruster']}:\")\n",
        "        print(f\"MAE: {tm['MAE']:.4f}  MSE: {tm['MSE']:.4f}\")\n",
        "        print(f\"RMSE: {tm['RMSE']:.4f}  R²: {tm['R2']:.4f}\")\n",
        "\n",
        "    visualize_results(targets, preds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
